{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 14:29:20.336745 139756668888896 file_utils.py:35] PyTorch version 1.6.0+cu92 available.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt \n",
    "from transformers import *\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "from model import *\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"\n",
    "pd.options.display.max_rows = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.read_csv('consistent.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 14:29:22.342936 139756668888896 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/champion/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "W0812 14:29:29.945687 139756668888896 tokenization_utils.py:1084] Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0812 14:29:30.005872 139756668888896 tokenization_utils.py:1084] Token indices sequence length is longer than the specified maximum sequence length for this model (673 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0812 14:29:34.357061 139756668888896 tokenization_utils.py:1084] Token indices sequence length is longer than the specified maximum sequence length for this model (775 > 512). Running this sequence through the model will result in indexing errors\n",
      "W0812 14:29:34.420893 139756668888896 tokenization_utils.py:1084] Token indices sequence length is longer than the specified maximum sequence length for this model (670 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "total_data['prop_value'] = total_data['prop_value'].apply(lambda x : x.lower())\n",
    "# 建立字典和分詞\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenize_concat_sentence_list = []\n",
    "\n",
    "for index, row in total_data.iterrows():\n",
    "    wiki_value = row['prop_value']\n",
    "    change_sentence = row['change_sentence']\n",
    "    \n",
    "    concat_sentence = []\n",
    "    concat_sentence.append(tokenizer.encode(['[CLS]'], add_special_tokens=False)[0])\n",
    "    wiki_value = tokenizer.encode(wiki_value, add_special_tokens=False)\n",
    "    change_sentence = tokenizer.encode(change_sentence, add_special_tokens=False)\n",
    "    change_sentence = change_sentence[:300]\n",
    "    for index in wiki_value:\n",
    "        concat_sentence.append(index)\n",
    "    concat_sentence.append(tokenizer.encode(['[SEP]'], add_special_tokens=False)[0])\n",
    "    for index in change_sentence:\n",
    "        concat_sentence.append(index)\n",
    "    \n",
    "    concat_sentence.append(tokenizer.encode(['[SEP]'], add_special_tokens=False)[0])\n",
    "    \n",
    "    tokenize_concat_sentence_list.append(torch.tensor(concat_sentence).cuda())\n",
    "    \n",
    "    \n",
    "train_concat_sentence, test_concat_sentence, y_train, y_test, train_index, test_index = train_test_split(tokenize_concat_sentence_list, total_data['label'], \n",
    "                                                                                                   total_data.index, test_size=0.2, random_state=42)\n",
    "train_concat_sentence, valid_concat_sentence, y_train, y_valid, train_index, valid_index = train_test_split(train_concat_sentence, y_train, train_index, test_size=0.25, random_state=42)\n",
    "\n",
    "train_concat_sentence = pad_sequence(train_concat_sentence, batch_first=True)\n",
    "y_train = torch.tensor(y_train.values).cuda()\n",
    "\n",
    "valid_concat_sentence = pad_sequence(valid_concat_sentence, batch_first=True)\n",
    "y_valid = torch.tensor(y_valid.values).cuda()\n",
    "\n",
    "test_concat_sentence = pad_sequence(test_concat_sentence, batch_first=True)\n",
    "y_test = torch.tensor(y_test.values).cuda()\n",
    "\n",
    "train_dataset = Data.TensorDataset(train_concat_sentence, y_train)\n",
    "valid_dataset = Data.TensorDataset(valid_concat_sentence, y_valid)\n",
    "test_dataset = Data.TensorDataset(test_concat_sentence, y_test)\n",
    "\n",
    "train_dataloader = Data.DataLoader(train_dataset, batch_size=32)\n",
    "valid_dataloader = Data.DataLoader(valid_dataset, batch_size=32)\n",
    "test_dataloader = Data.DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Fintuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:0,CE Loss:0.696039 train accuracy:0.501862 valid accuracy:0.491838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [02:34<49:01, 154.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:1,CE Loss:0.313723 train accuracy:0.876253 valid accuracy:0.936856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [05:08<46:20, 154.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:2,CE Loss:0.146290 train accuracy:0.958035 valid accuracy:0.958763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [07:40<43:36, 153.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:3,CE Loss:0.117708 train accuracy:0.964337 valid accuracy:0.956186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [10:13<40:54, 153.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:4,CE Loss:0.099761 train accuracy:0.968634 valid accuracy:0.960481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [12:45<38:15, 153.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:5,CE Loss:0.083527 train accuracy:0.974219 valid accuracy:0.969931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [15:18<35:41, 152.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:6,CE Loss:0.072830 train accuracy:0.975508 valid accuracy:0.965636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [17:50<33:06, 152.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:7,CE Loss:0.055479 train accuracy:0.982383 valid accuracy:0.971649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [20:23<30:32, 152.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:8,CE Loss:0.043918 train accuracy:0.984961 valid accuracy:0.971220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [22:55<27:59, 152.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:9,CE Loss:0.035534 train accuracy:0.988685 valid accuracy:0.975945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [25:28<25:26, 152.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:10,CE Loss:0.031770 train accuracy:0.990261 valid accuracy:0.976375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [28:00<22:53, 152.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:11,CE Loss:0.024542 train accuracy:0.991979 valid accuracy:0.975086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [30:33<20:19, 152.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:12,CE Loss:0.024999 train accuracy:0.992552 valid accuracy:0.972938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [33:04<17:45, 152.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:13,CE Loss:0.019484 train accuracy:0.994128 valid accuracy:0.974656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [35:36<15:12, 152.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:14,CE Loss:0.015788 train accuracy:0.995274 valid accuracy:0.974227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [38:07<12:39, 151.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:15,CE Loss:0.016715 train accuracy:0.994128 valid accuracy:0.966065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [40:39<10:07, 151.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:16,CE Loss:0.014409 train accuracy:0.996133 valid accuracy:0.974656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [43:11<07:35, 151.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:17,CE Loss:0.013178 train accuracy:0.996706 valid accuracy:0.976804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [45:42<05:03, 151.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:18,CE Loss:0.010314 train accuracy:0.997279 valid accuracy:0.971649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [48:14<02:31, 151.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:19,CE Loss:0.014498 train accuracy:0.995990 valid accuracy:0.971220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [50:45<00:00, 152.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# load bert model \n",
    "# bert-base-uncased : use English corpus pretrain\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-6, eps=1e-8)\n",
    "CE = nn.CrossEntropyLoss()\n",
    "epochs = 20\n",
    "\n",
    "valid_accuracy_list = []\n",
    "stopping_round = 0\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    save_finetune_model_name = 'bert_finetune_model'\n",
    "    CE_mean_loss = []\n",
    "    train_predict_result = []\n",
    "    valid_predict_result = []\n",
    "    model.train()\n",
    "    for train_concat_sentence_batch, y_train_batch in train_dataloader:\n",
    "        output = model(train_concat_sentence_batch)[0]\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        train_pred_prob = softmax(output)\n",
    "        train_pred_result = np.argmax(train_pred_prob.cpu().detach().numpy(), axis=1)\n",
    "        for result in train_pred_result:\n",
    "            train_predict_result.append(result)\n",
    "\n",
    "        CE_loss = CE(output, y_train_batch)\n",
    "        CE_mean_loss.append(CE_loss.cpu().detach().numpy())\n",
    "\n",
    "        CE_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    model.eval()\n",
    "    for valid_concat_sentence_batch , y_valid_batch in valid_dataloader:\n",
    "        output = model(valid_concat_sentence_batch)[0]\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "        valid_pred_prob = softmax(output)\n",
    "        valid_pred_result = np.argmax(valid_pred_prob.cpu().detach().numpy(), axis=1)\n",
    "        for result in valid_pred_result:\n",
    "            valid_predict_result.append(result)\n",
    "        \n",
    "    CE_mean_loss = np.mean(CE_mean_loss) \n",
    "    train_accuracy = accuracy_score(y_train.cpu(), train_predict_result)\n",
    "    valid_accuracy = accuracy_score(y_valid.cpu(), valid_predict_result)\n",
    "        \n",
    "    # early stopping\n",
    "    valid_accuracy_list.append(valid_accuracy)\n",
    "    flag = False\n",
    "    if epoch == 0:\n",
    "        pass\n",
    "    else:\n",
    "        if valid_accuracy < valid_accuracy_list[epoch-1]:\n",
    "            stopping_round += 1\n",
    "        else:\n",
    "            stopping_round -= 1\n",
    "    if stopping_round == 3:\n",
    "        print('Epochs:{},CE Loss:{:5f} train accuracy:{:5f} valid accuracy:{:5f}'.format(epoch,CE_mean_loss, train_accuracy, valid_accuracy))\n",
    "        model_save_path = '/home/champion/wiki/finetuned_model/bert_en'.format(epoch)\n",
    "        if not os.path.isdir(model_save_path):\n",
    "            os.mkdir(model_save_path)\n",
    "        model_structure_path = os.path.join(model_save_path, save_finetune_model_name)\n",
    "        torch.save(model.state_dict(), model_structure_path)\n",
    "        break\n",
    "\n",
    "    print('Epochs:{},CE Loss:{:5f} train accuracy:{:5f} valid accuracy:{:5f}'.format(epoch,CE_mean_loss, train_accuracy, valid_accuracy))\n",
    "    model_save_path = '/home/champion/wiki/finetuned_model/bert_en'.format(epoch)\n",
    "    if not os.path.isdir(model_save_path):\n",
    "        os.mkdir(model_save_path)\n",
    "    model_structure_path = os.path.join(model_save_path, save_finetune_model_name)\n",
    "    torch.save(model.state_dict(), model_structure_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0812 14:29:35.780840 139756668888896 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/champion/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0812 14:29:35.781648 139756668888896 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0812 14:29:36.887787 139756668888896 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/champion/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0812 14:29:40.277435 139756668888896 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0812 14:29:40.279312 139756668888896 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "all_finetuned_model_path = '/home/champion/wiki/finetuned_model/bert_en/bert_finetune_model'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased').cuda()\n",
    "pretrain_weight = torch.load(all_finetuned_model_path, map_location=device)\n",
    "model.load_state_dict(pretrain_weight,strict=False)\n",
    "\n",
    "test_predict_result = []\n",
    "test_predict_prob = []\n",
    "model.eval()\n",
    "for test_concat_sentence_batch, y_test_batch in test_dataloader:\n",
    "    output = model(test_concat_sentence_batch)[0]\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    test_pred_prob = softmax(output)\n",
    "    test_pred_result = np.argmax(test_pred_prob.cpu().detach().numpy(), axis=1)\n",
    "    test_pred_prob = torch.max(test_pred_prob, axis=1).values.cpu().detach().numpy()\n",
    "    for result in test_pred_result:\n",
    "        test_predict_result.append(result)\n",
    "    for prob in test_pred_prob:\n",
    "        test_predict_prob.append(prob)\n",
    "test_predict_result = np.array(test_predict_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 180x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9586776859504132\n",
      "fpr: 0.08196721311475409\n",
      "tpr: 1.0\n",
      "rec: 0.9230769230769231\n",
      "pre: 1.0\n",
      "f1_score: 0.9600000000000001\n"
     ]
    }
   ],
   "source": [
    "prop = pd.DataFrame({'True':y_test.cpu().numpy(),'Pred':test_predict_result, 'Pred_prob':test_predict_prob,\n",
    "                     'prop_label':total_data.iloc[test_index]['prop_label']})\n",
    "io = prop.loc[prop['prop_label']=='Patientplus ID']\n",
    "\n",
    "\n",
    "confmat = confusion_matrix(y_true = io['True'],y_pred=io['Pred'])\n",
    "fig,ax = plt.subplots(figsize=(2.5,2.5))\n",
    "ax.matshow(confmat,cmap=plt.cm.Blues,alpha=0.3)\n",
    "for i in range(confmat.shape[0]):\n",
    "    for j in range(confmat.shape[1]):\n",
    "        ax.text(x=j,y=i,s=confmat[i,j],va='center',ha='center')\n",
    "plt.xlabel('predicted label')\n",
    "plt.ylabel('true label')\n",
    "plt.show()\n",
    "\n",
    "acc = (confmat[0,0] + confmat[1,1]) / (confmat[0,0] + confmat[0,1] + confmat[1,0] + confmat[1,1])\n",
    "fpr = (confmat[0,1]) / (confmat[0,0] + confmat[0,1])\n",
    "tpr = (confmat[1,1]) / (confmat[1,0] + confmat[1,1])\n",
    "rec = (confmat[1,1] ) / (confmat[1,0] + confmat[1,1])\n",
    "pre = (confmat[1,1]) / (confmat[0,1] + confmat[1,1])\n",
    "f1_score = (2*pre*rec) / (pre + rec)\n",
    "print('Accuracy:',acc)\n",
    "print('fpr:',fpr)\n",
    "print('tpr:',tpr)\n",
    "print('rec:',pre)\n",
    "print('pre:',rec)\n",
    "print('f1_score:',f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
